{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HJbgk12/2.-AI-Goorm-QA-model/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktjT0rGibHa5",
        "outputId": "a31e937b-b331-4990-8704-13faa91694d9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsDhWYEDr9mX",
        "outputId": "a41bcb6f-fb99-46e0-8537-bff918e2ded6"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8KavUE7tJPn",
        "outputId": "c5f2c358-78fb-4909-e67f-6bfb77e7d72b"
      },
      "source": [
        "%cd /content/drive/MyDrive/KoreanMRC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KoreanMRC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEst8ENTklpk"
      },
      "source": [
        "! apt-get install -y g++ openjdk-8-jdk python3-dev curl git build-essential\n",
        "! pip install konlpy \"tweepy<4.0.0\"\n",
        "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruzoeyi9mYJ6"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-PGXvdzZaW-",
        "outputId": "aa1eaa9e-1c82-49c6-eaf9-ddf6ea810ba6"
      },
      "source": [
        "!git clone https://github.com/SKTBrain/KoBERT.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'KoBERT' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkjnFpo5gnzE"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9_yI-aSklpq"
      },
      "source": [
        "import torch\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import BertConfig\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "#from KoBERT.kobert_hf.kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "import ko_mrc.datasets as datasets\n",
        "import ko_mrc.utils as utils\n",
        "import ko_mrc.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsbQwSMkklps",
        "outputId": "d715320a-ef3f-469e-e531-09ef8bce6e08"
      },
      "source": [
        "dataset = datasets.KoMRC.load('data/train3.json')\n",
        "print(\"Number of Questions\", len(dataset))\n",
        "print(dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Questions 255462\n",
            "{'guid': '798db07f0b9046759deed9d4a35ce31e', 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.', 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?', 'answers': [{'text': '한 달가량', 'answer_start': 478}, {'text': '한 달', 'answer_start': 478}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNLLM7TMklpu",
        "outputId": "62048b9e-1a29-490f-d3b9-89afd5b5733e"
      },
      "source": [
        "dataset = datasets.TokenizedKoMRC.load('data/train3.json')\n",
        "train_dataset, eval_dataset = datasets.TokenizedKoMRC.split(dataset)\n",
        "print(\"Number of Questions\", len(train_dataset), len(eval_dataset))\n",
        "print(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Questions 229916 25546\n",
            "{'guid': '86cf7f16da224f81a7a86b61ae2d539f', 'context_original': '일본에서는 현재 교사의 40% 이상이 50대인데요. 경험 많은 교사들이 조만간 대규모로 퇴직을 하게 돼 지자체마다 우수한 교사를 확보하기 위한 쟁탈전을 벌이고 있습니다. [리포트] 지난해 12월 후쿠오카 현 교육위원회가 도쿄에서 실시한 채용시험입니다. 대상은 현직 교사. 응시자의 90%가 합격해 올봄부터 후쿠오카에서 근무하게 됐습니다. 교사 수가 많은 도쿄 등 수도권에서 이른바 스카우트를 하는 것입니다. 부모 간병이나 육아 등을 위해 고향으로 돌아가려는 사람들이 지원한다고 합니다. 전국 교사 채용 수는 1982년을 정점으로 계속 줄어 2000년에는 5분의 1까지 줄었는데요. 당시 퇴직자 수가 적고 저출산으로 교사를 늘릴 필요가 없었다는 게 이유로 지적됩니다. 경력 교사를 빼앗기는 지역에서는 고민이 많습니다.[가나가와현 교육위원회 : 가장 활약할 나이의 교사가 빠져나가면 학교 운영에 어려움이 생깁니다.] 전국의 교사 인원은 법률로 정해져 있어 지자체 차원에서는 근본적인 해결이 어려운데요. 교사 쟁탈전이 가열되면 지역에 따라 교육의 질에 편차가 생길 수 있다고 우려하고 있습니다.', 'context_position': [(0, 2), (2, 4), (4, 5), (6, 8), (9, 11), (11, 12), (13, 15), (15, 16), (17, 19), (19, 20), (21, 23), (23, 24), (24, 27), (27, 28), (29, 31), (32, 33), (33, 34), (35, 37), (37, 38), (38, 39), (40, 43), (44, 45), (45, 47), (47, 48), (49, 51), (51, 52), (53, 54), (54, 55), (56, 57), (58, 61), (61, 63), (64, 66), (66, 67), (68, 70), (70, 71), (72, 74), (74, 75), (75, 76), (77, 79), (80, 83), (83, 84), (85, 87), (87, 88), (89, 90), (90, 93), (93, 94), (95, 96), (96, 99), (99, 100), (101, 104), (105, 107), (107, 108), (109, 113), (114, 115), (116, 118), (118, 121), (121, 122), (123, 125), (125, 127), (128, 130), (130, 131), (132, 134), (134, 136), (136, 139), (139, 140), (141, 143), (143, 144), (145, 147), (148, 150), (150, 151), (152, 155), (155, 156), (157, 159), (159, 160), (160, 161), (162, 164), (164, 165), (166, 167), (167, 168), (168, 170), (171, 175), (175, 177), (178, 180), (180, 181), (181, 182), (183, 184), (184, 187), (187, 188), (189, 191), (192, 193), (193, 194), (195, 196), (196, 197), (198, 200), (201, 202), (203, 206), (206, 208), (209, 212), (213, 217), (217, 218), (219, 220), (220, 221), (222, 223), (223, 226), (226, 227), (228, 230), (231, 233), (233, 235), (236, 238), (239, 240), (240, 241), (242, 244), (245, 247), (247, 249), (250, 253), (253, 255), (256, 258), (258, 259), (259, 260), (261, 263), (263, 266), (267, 270), (270, 271), (272, 274), (275, 277), (278, 280), (281, 282), (282, 283), (284, 288), (288, 289), (289, 290), (291, 293), (293, 295), (296, 298), (299, 300), (300, 301), (302, 306), (306, 307), (307, 308), (308, 309), (310, 311), (311, 312), (312, 313), (314, 315), (315, 317), (318, 319), (319, 320), (320, 323), (323, 324), (325, 327), (328, 331), (332, 333), (333, 334), (335, 336), (336, 337), (338, 341), (341, 343), (344, 346), (346, 347), (348, 350), (351, 353), (353, 354), (355, 356), (356, 357), (357, 359), (360, 361), (362, 364), (364, 365), (366, 368), (368, 371), (371, 372), (373, 375), (376, 378), (378, 379), (380, 383), (383, 384), (385, 387), (387, 389), (389, 390), (391, 393), (393, 394), (395, 396), (396, 399), (399, 400), (400, 401), (401, 406), (407, 409), (409, 412), (413, 414), (415, 417), (418, 420), (420, 421), (422, 424), (424, 425), (426, 428), (428, 429), (430, 434), (434, 435), (436, 438), (439, 441), (441, 442), (443, 446), (446, 447), (448, 452), (452, 453), (453, 454), (455, 457), (457, 458), (459, 461), (462, 464), (464, 465), (466, 468), (468, 469), (470, 473), (474, 475), (475, 476), (477, 480), (481, 483), (483, 485), (485, 486), (487, 489), (489, 490), (490, 491), (492, 494), (494, 495), (496, 499), (499, 500), (500, 501), (501, 502), (503, 505), (506, 509), (509, 510), (511, 513), (513, 514), (514, 515), (516, 518), (518, 519), (520, 522), (523, 525), (525, 526), (527, 528), (528, 529), (530, 532), (532, 533), (534, 536), (537, 538), (539, 540), (540, 542), (543, 545), (545, 546), (546, 547), (548, 549), (549, 552), (552, 553)], 'question_original': '일본지자체들이 뭘 확보하기 위해서 쟁탈전을 벌이고 있어?', 'context': ['일본', '에서', '는', '현재', '교사', '의', '40', '%', '이상', '이', '50', '대', '인데요', '.', '경험', '많', '은', '교사', '들', '이', '조만간', '대', '규모', '로', '퇴직', '을', '하', '게', '돼', '지자체', '마다', '우수', '한', '교사', '를', '확보', '하', '기', '위한', '쟁탈전', '을', '벌이', '고', '있', '습니다', '.', '[', '리포트', ']', '지난해', '12', '월', '후쿠오카', '현', '교육', '위원회', '가', '도쿄', '에서', '실시', '한', '채용', '시험', '입니다', '.', '대상', '은', '현직', '교사', '.', '응시자', '의', '90', '%', '가', '합격', '해', '올', '봄', '부터', '후쿠오카', '에서', '근무', '하', '게', '됐', '습니다', '.', '교사', '수', '가', '많', '은', '도쿄', '등', '수도권', '에서', '이른바', '스카우트', '를', '하', '는', '것', '입니다', '.', '부모', '간병', '이나', '육아', '등', '을', '위해', '고향', '으로', '돌아가', '려는', '사람', '들', '이', '지원', '한다고', '합니다', '.', '전국', '교사', '채용', '수', '는', '1982', '년', '을', '정점', '으로', '계속', '줄', '어', '2000', '년', '에', '는', '5', '분', '의', '1', '까지', '줄', '었', '는데요', '.', '당시', '퇴직자', '수', '가', '적', '고', '저출산', '으로', '교사', '를', '늘릴', '필요', '가', '없', '었', '다는', '게', '이유', '로', '지적', '됩니다', '.', '경력', '교사', '를', '빼앗기', '는', '지역', '에서', '는', '고민', '이', '많', '습니다', '.', '[', '가나가와현', '교육', '위원회', ':', '가장', '활약', '할', '나이', '의', '교사', '가', '빠져나가', '면', '학교', '운영', '에', '어려움', '이', '생깁니다', '.', ']', '전국', '의', '교사', '인원', '은', '법률', '로', '정해져', '있', '어', '지자체', '차원', '에서', '는', '근본', '적', '인', '해결', '이', '어려운', '데', '요', '.', '교사', '쟁탈전', '이', '가열', '되', '면', '지역', '에', '따라', '교육', '의', '질', '에', '편차', '가', '생길', '수', '있', '다고', '우려', '하', '고', '있', '습니다', '.'], 'question': ['일본', '지자체', '들', '이', '뭘', '확보', '하', '기', '위해서', '쟁탈전', '을', '벌이', '고', '있', '어', '?'], 'answers': [{'start': 31, 'end': 34}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOLsHtmxklpv",
        "outputId": "732eaaa9-b5f6-4ed2-ebd5-e8327bb710b0"
      },
      "source": [
        "sample = train_dataset[0]\n",
        "print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['우수', '한', '교사', '를']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHewCq15klpw",
        "outputId": "0b5d14f0-af42-4cfd-f0ce-455f5f2a838b"
      },
      "source": [
        "tokenizer = utils.Tokenizer.build_vocab(dataset)\n",
        "print(tokenizer.sample2ids(train_dataset[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Counting Vocab: 100%|██████████| 255462/255462 [10:12<00:00, 417.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'guid': '86cf7f16da224f81a7a86b61ae2d539f', 'context': '일본에서는 현재 교사의 40% 이상이 50대인데요. 경험 많은 교사들이 조만간 대규모로 퇴직을 하게 돼 지자체마다 우수한 교사를 확보하기 위한 쟁탈전을 벌이고 있습니다. [리포트] 지난해 12월 후쿠오카 현 교육위원회가 도쿄에서 실시한 채용시험입니다. 대상은 현직 교사. 응시자의 90%가 합격해 올봄부터 후쿠오카에서 근무하게 됐습니다. 교사 수가 많은 도쿄 등 수도권에서 이른바 스카우트를 하는 것입니다. 부모 간병이나 육아 등을 위해 고향으로 돌아가려는 사람들이 지원한다고 합니다. 전국 교사 채용 수는 1982년을 정점으로 계속 줄어 2000년에는 5분의 1까지 줄었는데요. 당시 퇴직자 수가 적고 저출산으로 교사를 늘릴 필요가 없었다는 게 이유로 지적됩니다. 경력 교사를 빼앗기는 지역에서는 고민이 많습니다.[가나가와현 교육위원회 : 가장 활약할 나이의 교사가 빠져나가면 학교 운영에 어려움이 생깁니다.] 전국의 교사 인원은 법률로 정해져 있어 지자체 차원에서는 근본적인 해결이 어려운데요. 교사 쟁탈전이 가열되면 지역에 따라 교육의 질에 편차가 생길 수 있다고 우려하고 있습니다.', 'question': '일본지자체들이 뭘 확보하기 위해서 쟁탈전을 벌이고 있어?', 'position': [(0, 2), (2, 4), (4, 5), (6, 8), (9, 11), (11, 12), (13, 15), (15, 16), (17, 19), (19, 20), (21, 23), (23, 24), (24, 27), (27, 28), (29, 31), (32, 33), (33, 34), (35, 37), (37, 38), (38, 39), (40, 43), (44, 45), (45, 47), (47, 48), (49, 51), (51, 52), (53, 54), (54, 55), (56, 57), (58, 61), (61, 63), (64, 66), (66, 67), (68, 70), (70, 71), (72, 74), (74, 75), (75, 76), (77, 79), (80, 83), (83, 84), (85, 87), (87, 88), (89, 90), (90, 93), (93, 94), (95, 96), (96, 99), (99, 100), (101, 104), (105, 107), (107, 108), (109, 113), (114, 115), (116, 118), (118, 121), (121, 122), (123, 125), (125, 127), (128, 130), (130, 131), (132, 134), (134, 136), (136, 139), (139, 140), (141, 143), (143, 144), (145, 147), (148, 150), (150, 151), (152, 155), (155, 156), (157, 159), (159, 160), (160, 161), (162, 164), (164, 165), (166, 167), (167, 168), (168, 170), (171, 175), (175, 177), (178, 180), (180, 181), (181, 182), (183, 184), (184, 187), (187, 188), (189, 191), (192, 193), (193, 194), (195, 196), (196, 197), (198, 200), (201, 202), (203, 206), (206, 208), (209, 212), (213, 217), (217, 218), (219, 220), (220, 221), (222, 223), (223, 226), (226, 227), (228, 230), (231, 233), (233, 235), (236, 238), (239, 240), (240, 241), (242, 244), (245, 247), (247, 249), (250, 253), (253, 255), (256, 258), (258, 259), (259, 260), (261, 263), (263, 266), (267, 270), (270, 271), (272, 274), (275, 277), (278, 280), (281, 282), (282, 283), (284, 288), (288, 289), (289, 290), (291, 293), (293, 295), (296, 298), (299, 300), (300, 301), (302, 306), (306, 307), (307, 308), (308, 309), (310, 311), (311, 312), (312, 313), (314, 315), (315, 317), (318, 319), (319, 320), (320, 323), (323, 324), (325, 327), (328, 331), (332, 333), (333, 334), (335, 336), (336, 337), (338, 341), (341, 343), (344, 346), (346, 347), (348, 350), (351, 353), (353, 354), (355, 356), (356, 357), (357, 359), (360, 361), (362, 364), (364, 365), (366, 368), (368, 371), (371, 372), (373, 375), (376, 378), (378, 379), (380, 383), (383, 384), (385, 387), (387, 389), (389, 390), (391, 393), (393, 394), (395, 396), (396, 399), (399, 400), (400, 401), (401, 406), (407, 409), (409, 412), (413, 414), (415, 417), (418, 420), (420, 421), (422, 424), (424, 425), (426, 428), (428, 429), (430, 434), (434, 435), (436, 438), (439, 441), (441, 442), (443, 446), (446, 447), (448, 452), (452, 453), (453, 454), (455, 457), (457, 458), (459, 461), (462, 464), (464, 465), (466, 468), (468, 469), (470, 473), (474, 475), (475, 476), (477, 480), (481, 483), (483, 485), (485, 486), (487, 489), (489, 490), (490, 491), (492, 494), (494, 495), (496, 499), (499, 500), (500, 501), (501, 502), (503, 505), (506, 509), (509, 510), (511, 513), (513, 514), (514, 515), (516, 518), (518, 519), (520, 522), (523, 525), (525, 526), (527, 528), (528, 529), (530, 532), (532, 533), (534, 536), (537, 538), (539, 540), (540, 542), (543, 545), (545, 546), (546, 547), (548, 549), (549, 552), (552, 553)], 'input_ids': [2, 519, 14759, 267, 31, 12266, 411, 58, 485, 2747, 2053, 99, 2054, 246, 39, 110, 173, 3, 519, 11, 40, 1130, 7188, 42, 697, 702, 548, 31, 518, 653, 67003, 15, 905, 59, 20, 7188, 267, 31, 4937, 653, 1566, 145, 5493, 99, 58, 118, 166, 14759, 1452, 1152, 73, 7188, 84, 411, 58, 485, 745, 2053, 99, 2054, 246, 39, 2409, 15, 13884, 27200, 12023, 68, 602, 133, 556, 2604, 1103, 1594, 7, 1578, 11, 1196, 73, 3500, 1880, 721, 15, 260, 20, 12201, 7188, 15, 49643, 42, 3097, 702, 7, 1881, 95, 4, 3433, 116, 556, 11, 1878, 58, 118, 13, 2409, 15, 7188, 410, 7, 59, 20, 1578, 17, 15049, 11, 7984, 5804, 84, 58, 40, 101, 721, 15, 6497, 28538, 817, 9164, 17, 99, 378, 5367, 44, 5525, 4653, 970, 267, 31, 193, 1619, 2404, 15, 257, 7188, 3500, 410, 40, 2360, 129, 99, 10006, 44, 457, 100, 110, 785, 129, 33, 40, 249, 1033, 42, 289, 98, 100, 136, 40291, 15, 1340, 29002, 410, 7, 148, 246, 24966, 44, 7188, 84, 3093, 909, 7, 170, 136, 591, 118, 1234, 145, 1651, 2419, 15, 868, 7188, 84, 10632, 40, 49, 11, 40, 1322, 31, 59, 2409, 15, 13884, 28020, 1103, 1594, 1009, 1083, 5749, 415, 2848, 42, 7188, 7, 7389, 35, 1147, 1440, 33, 4473, 31, 46856, 15, 12023, 257, 42, 7188, 3359, 20, 1887, 145, 10220, 39, 110, 14759, 447, 11, 40, 1339, 148, 201, 763, 31, 454, 502, 4285, 15, 7188, 2053, 31, 22638, 82, 35, 49, 33, 103, 1103, 42, 811, 33, 24197, 7, 4845, 410, 39, 822, 1229, 58, 246, 39, 2409, 15, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start': 49, 'end': 52}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDaZDV0wklpy",
        "outputId": "75e3fa7a-2334-4027-e34a-e477283fb2f6"
      },
      "source": [
        "train_dataset = utils.TokenizerWrapperDataset(train_dataset, tokenizer)\n",
        "eval_dataset = utils.TokenizerWrapperDataset(eval_dataset, tokenizer)\n",
        "\n",
        "print(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'guid': '86cf7f16da224f81a7a86b61ae2d539f', 'context': '일본에서는 현재 교사의 40% 이상이 50대인데요. 경험 많은 교사들이 조만간 대규모로 퇴직을 하게 돼 지자체마다 우수한 교사를 확보하기 위한 쟁탈전을 벌이고 있습니다. [리포트] 지난해 12월 후쿠오카 현 교육위원회가 도쿄에서 실시한 채용시험입니다. 대상은 현직 교사. 응시자의 90%가 합격해 올봄부터 후쿠오카에서 근무하게 됐습니다. 교사 수가 많은 도쿄 등 수도권에서 이른바 스카우트를 하는 것입니다. 부모 간병이나 육아 등을 위해 고향으로 돌아가려는 사람들이 지원한다고 합니다. 전국 교사 채용 수는 1982년을 정점으로 계속 줄어 2000년에는 5분의 1까지 줄었는데요. 당시 퇴직자 수가 적고 저출산으로 교사를 늘릴 필요가 없었다는 게 이유로 지적됩니다. 경력 교사를 빼앗기는 지역에서는 고민이 많습니다.[가나가와현 교육위원회 : 가장 활약할 나이의 교사가 빠져나가면 학교 운영에 어려움이 생깁니다.] 전국의 교사 인원은 법률로 정해져 있어 지자체 차원에서는 근본적인 해결이 어려운데요. 교사 쟁탈전이 가열되면 지역에 따라 교육의 질에 편차가 생길 수 있다고 우려하고 있습니다.', 'question': '일본지자체들이 뭘 확보하기 위해서 쟁탈전을 벌이고 있어?', 'position': [(0, 2), (2, 4), (4, 5), (6, 8), (9, 11), (11, 12), (13, 15), (15, 16), (17, 19), (19, 20), (21, 23), (23, 24), (24, 27), (27, 28), (29, 31), (32, 33), (33, 34), (35, 37), (37, 38), (38, 39), (40, 43), (44, 45), (45, 47), (47, 48), (49, 51), (51, 52), (53, 54), (54, 55), (56, 57), (58, 61), (61, 63), (64, 66), (66, 67), (68, 70), (70, 71), (72, 74), (74, 75), (75, 76), (77, 79), (80, 83), (83, 84), (85, 87), (87, 88), (89, 90), (90, 93), (93, 94), (95, 96), (96, 99), (99, 100), (101, 104), (105, 107), (107, 108), (109, 113), (114, 115), (116, 118), (118, 121), (121, 122), (123, 125), (125, 127), (128, 130), (130, 131), (132, 134), (134, 136), (136, 139), (139, 140), (141, 143), (143, 144), (145, 147), (148, 150), (150, 151), (152, 155), (155, 156), (157, 159), (159, 160), (160, 161), (162, 164), (164, 165), (166, 167), (167, 168), (168, 170), (171, 175), (175, 177), (178, 180), (180, 181), (181, 182), (183, 184), (184, 187), (187, 188), (189, 191), (192, 193), (193, 194), (195, 196), (196, 197), (198, 200), (201, 202), (203, 206), (206, 208), (209, 212), (213, 217), (217, 218), (219, 220), (220, 221), (222, 223), (223, 226), (226, 227), (228, 230), (231, 233), (233, 235), (236, 238), (239, 240), (240, 241), (242, 244), (245, 247), (247, 249), (250, 253), (253, 255), (256, 258), (258, 259), (259, 260), (261, 263), (263, 266), (267, 270), (270, 271), (272, 274), (275, 277), (278, 280), (281, 282), (282, 283), (284, 288), (288, 289), (289, 290), (291, 293), (293, 295), (296, 298), (299, 300), (300, 301), (302, 306), (306, 307), (307, 308), (308, 309), (310, 311), (311, 312), (312, 313), (314, 315), (315, 317), (318, 319), (319, 320), (320, 323), (323, 324), (325, 327), (328, 331), (332, 333), (333, 334), (335, 336), (336, 337), (338, 341), (341, 343), (344, 346), (346, 347), (348, 350), (351, 353), (353, 354), (355, 356), (356, 357), (357, 359), (360, 361), (362, 364), (364, 365), (366, 368), (368, 371), (371, 372), (373, 375), (376, 378), (378, 379), (380, 383), (383, 384), (385, 387), (387, 389), (389, 390), (391, 393), (393, 394), (395, 396), (396, 399), (399, 400), (400, 401), (401, 406), (407, 409), (409, 412), (413, 414), (415, 417), (418, 420), (420, 421), (422, 424), (424, 425), (426, 428), (428, 429), (430, 434), (434, 435), (436, 438), (439, 441), (441, 442), (443, 446), (446, 447), (448, 452), (452, 453), (453, 454), (455, 457), (457, 458), (459, 461), (462, 464), (464, 465), (466, 468), (468, 469), (470, 473), (474, 475), (475, 476), (477, 480), (481, 483), (483, 485), (485, 486), (487, 489), (489, 490), (490, 491), (492, 494), (494, 495), (496, 499), (499, 500), (500, 501), (501, 502), (503, 505), (506, 509), (509, 510), (511, 513), (513, 514), (514, 515), (516, 518), (518, 519), (520, 522), (523, 525), (525, 526), (527, 528), (528, 529), (530, 532), (532, 533), (534, 536), (537, 538), (539, 540), (540, 542), (543, 545), (545, 546), (546, 547), (548, 549), (549, 552), (552, 553)], 'input_ids': tensor([    2,   519, 14759,   267,    31, 12266,   411,    58,   485,  2747,\n",
            "         2053,    99,  2054,   246,    39,   110,   173,     3,   519,    11,\n",
            "           40,  1130,  7188,    42,   697,   702,   548,    31,   518,   653,\n",
            "        67003,    15,   905,    59,    20,  7188,   267,    31,  4937,   653,\n",
            "         1566,   145,  5493,    99,    58,   118,   166, 14759,  1452,  1152,\n",
            "           73,  7188,    84,   411,    58,   485,   745,  2053,    99,  2054,\n",
            "          246,    39,  2409,    15, 13884, 27200, 12023,    68,   602,   133,\n",
            "          556,  2604,  1103,  1594,     7,  1578,    11,  1196,    73,  3500,\n",
            "         1880,   721,    15,   260,    20, 12201,  7188,    15, 49643,    42,\n",
            "         3097,   702,     7,  1881,    95,     4,  3433,   116,   556,    11,\n",
            "         1878,    58,   118,    13,  2409,    15,  7188,   410,     7,    59,\n",
            "           20,  1578,    17, 15049,    11,  7984,  5804,    84,    58,    40,\n",
            "          101,   721,    15,  6497, 28538,   817,  9164,    17,    99,   378,\n",
            "         5367,    44,  5525,  4653,   970,   267,    31,   193,  1619,  2404,\n",
            "           15,   257,  7188,  3500,   410,    40,  2360,   129,    99, 10006,\n",
            "           44,   457,   100,   110,   785,   129,    33,    40,   249,  1033,\n",
            "           42,   289,    98,   100,   136, 40291,    15,  1340, 29002,   410,\n",
            "            7,   148,   246, 24966,    44,  7188,    84,  3093,   909,     7,\n",
            "          170,   136,   591,   118,  1234,   145,  1651,  2419,    15,   868,\n",
            "         7188,    84, 10632,    40,    49,    11,    40,  1322,    31,    59,\n",
            "         2409,    15, 13884, 28020,  1103,  1594,  1009,  1083,  5749,   415,\n",
            "         2848,    42,  7188,     7,  7389,    35,  1147,  1440,    33,  4473,\n",
            "           31, 46856,    15, 12023,   257,    42,  7188,  3359,    20,  1887,\n",
            "          145, 10220,    39,   110, 14759,   447,    11,    40,  1339,   148,\n",
            "          201,   763,    31,   454,   502,  4285,    15,  7188,  2053,    31,\n",
            "        22638,    82,    35,    49,    33,   103,  1103,    42,   811,    33,\n",
            "        24197,     7,  4845,   410,    39,   822,  1229,    58,   246,    39,\n",
            "         2409,    15,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1]), 'start': 49, 'end': 52, 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ylvp8LBklpz",
        "outputId": "eeab9be9-f7d1-4ed3-8384-d8f8c078969b"
      },
      "source": [
        "collator = utils.Collator(tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator, num_workers=1)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False, collate_fn=collator, num_workers=1)\n",
        "batch = next(iter(train_loader))\n",
        "print(batch['input_ids'].shape)\n",
        "print(batch['input_ids'])\n",
        "print(list(batch.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 706])\n",
            "tensor([[    2,  1484,  1664,  ...,     0,     0,     0],\n",
            "        [    2,  4078, 35468,  ...,     0,     0,     0],\n",
            "        [    2, 31299, 31300,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    2, 43157,    40,  ...,     0,     0,     0],\n",
            "        [    2,   798,     7,  ...,     0,     0,     0],\n",
            "        [    2, 13054,  9884,  ...,    15,  3452,     3]])\n",
            "['guid', 'context', 'question', 'position', 'input_ids', 'token_type_ids', 'start', 'end', 'attention_mask']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo1ZqmTUklp2",
        "outputId": "353ec449-24e6-4004-da35-6a5624e53605"
      },
      "source": [
        "config = BertConfig(\n",
        "     vocab_size=tokenizer.vocab_size,\n",
        "     max_position_embeddings=1024,\n",
        "     hidden_size=256,\n",
        "     num_hidden_layers=4,\n",
        "     num_attention_heads=4,\n",
        "     intermediate_size=1024\n",
        ")\n",
        "model = models.BertForQuestionAnswering(config)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(122633, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(1024, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (start_linear): Linear(in_features=768, out_features=1, bias=True)\n",
              "  (end_linear): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmnnlrGrwX-8"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmeqMmjGklp4"
      },
      "source": [
        "writer = SummaryWriter(log_dir='review_train4')\n",
        "os.makedirs('dump8', exist_ok=True)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-DmZwaDklp5"
      },
      "source": [
        "for epoch in range(1, 31):\n",
        "    print(\"Epoch\", epoch)\n",
        "    for batch in tqdm(train_loader, desc='Train'):\n",
        "        del batch['guid'], batch['context'], batch['question'], batch['position']\n",
        "        batch = {key: value.cuda() for key, value in batch.items()}\n",
        "        start = batch.pop('start')\n",
        "        end = batch.pop('end')\n",
        "        \n",
        "        start_logits, end_logits = model(**batch)\n",
        "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "        writer.add_scalar('Train Loss', loss.item(), step)\n",
        "        del batch, start, end, start_logits, end_logits, loss\n",
        "\n",
        "    with torch.no_grad():\n",
        "        losses = []\n",
        "        for batch in tqdm(eval_loader, desc=\"Evaluation\"):\n",
        "            del batch['guid'], batch['context'], batch['question'], batch['position']\n",
        "            batch = {key: value.cuda() for key, value in batch.items()}\n",
        "            start = batch.pop('start')\n",
        "            end = batch.pop('end')\n",
        "\n",
        "            start_logits, end_logits = model(**batch)\n",
        "            loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            del batch, start, end, start_logits, end_logits, loss\n",
        "        loss = sum(losses) / len(losses)\n",
        "        writer.add_scalar('Eval Loss', loss, step)\n",
        "\n",
        "    model.save_pretrained(f'dump8/model.{epoch}')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOpRXTuaklp7",
        "outputId": "3f742031-2ccd-45c6-ee52-5e64743b519b"
      },
      "source": [
        "model = models.BertForQuestionAnswering.from_pretrained('dump8/model.30')\n",
        "model.cuda()\n",
        "model.eval() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(117152, 256, padding_idx=0)\n",
              "      (position_embeddings): Embedding(1024, 256)\n",
              "      (token_type_embeddings): Embedding(2, 256)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (start_linear): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (end_linear): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GdEkdPL941J"
      },
      "source": [
        "텐서보드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx_mnWiH7uD-"
      },
      "source": [
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLAoocao8TPz"
      },
      "source": [
        "log_dir = \"log/my_board/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwajiFde_sM-"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "PAnTEM1nAD2N",
        "outputId": "26dc86c3-7a79-4fb9-81c6-62476b22352e"
      },
      "source": [
        "%tensorboard --logdir reviewAIhub/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FQ9Q8e4klp8",
        "outputId": "f1676b84-fc9b-4796-b54d-0963200a1aa3"
      },
      "source": [
        "for idx, raw_sample in zip(range(1, 4), train_dataset):\n",
        "    sample = dict(raw_sample) \n",
        "    context = sample.pop('context')\n",
        "    question = sample.pop('question')\n",
        "    position = sample.pop('position')\n",
        "    start, end = sample.pop('start'), sample.pop('end')\n",
        "    del sample['guid']\n",
        "\n",
        "    sample = {key: value.cuda()[None, :] for key, value in sample.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_logits, end_logits = model(**sample)\n",
        "    start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
        "\n",
        "    print(f'------{idx}------')\n",
        "    print('Context:', context)\n",
        "    print('Question:', question)\n",
        "    print('Answer:', tokenizer.logits2answer(raw_sample, start_logits, end_logits))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------1------\n",
            "Context: 현대사진의 선각자로 평가받는 로버트 프랭크, ‘소나무 작가’ 배병우, 일본 작가 히로시 스키모토, 인물·다큐멘터리 사진작가 스티브 매커리, 몽상적인 내용을 사진에 담는 베르나르 포콩, 벌거벗은 젊은이들 사진으로 유명한 라이언 맥긴리…. 늦가을 화랑가에 국내외 유명 사진작가의 작품전이 줄을 잇고 있다. ‘잡화점식’ 전시에서 벗어나 각기 다른 테마를 다룬 기획전이어서 더욱 주목된다. 디지털 문화의 확산으로 사진과 그림의 경계가 허물어지는 경향도 확인할 수 있다. ○히로시·프랭크 등 거장 총출동도쿄와 뉴욕, 유럽을 주 무대로 활동하는 ‘밀리언달러’ 사진작가 히로시(66)의 개인전은 내달 5일 서울 한남동 삼성미술관 리움에서 개막한다. 히로시는 시간과 공간이라는 주제와 흑백사진의 표현을 연구해온 작가. 특정한 순간을 포착하는 대신 한 개의 프레임에 전체 작품을 찍어서 시간의 흐름을 연결한 게 특징이다. 이번 전시에는 ‘극장’ ‘바다풍경’ ‘초상’ 시리즈 등 100여점을 내보인다.스위스 출신 사진작가 프랭크(89)의 작품전은 서울 방이동 한미사진미술관에서 오는 9일부터 내년 2월9일까지 이어진다. 1958년작 ‘미국인’을 비롯해 1970~90년대 폴라로이드 작품 등 프랭크의 70년 사진 인생을 한눈에 보여주는 오리지널 프린트 115점이 걸린다. 7일부터 서울 통의동 대림미술관에서 열리는 ‘청춘, 그 찬란한 기록’전은 미국 유망 사진작가 라이언 맥긴리(25)의 국내 첫 개인전이다. 자유와 불안, 방황, 일탈, 열정 등 젊은이들의 내면에 공존하는 감정을 렌즈로 솔직하게 잡아낸 대표작 100여점을 만날 수 있다. 맥긴리는 25세의 젊은 나이에 미국 휘트니미술관과 뉴욕현대미술관에서 개인전을 열어 세계 사진계의 주목을 받고 있다. 프랑스 사진작가 포콩의 몽상적인 내용을 담은 신작들은 오는 9일부터 부산 고은사진미술관에 걸린다. 유년시절 체험한 사랑과 열정, 외로움 등을 푸른색으로 녹여낸 작품이 눈길을 끈다. 이 밖에 필립 할스만(세종문화회관)의 작품전, 이명호(38·중국 베이징 798포토갤러리), 김태동(35·일우스페이스), 오상택(예화랑) 씨의 개인전, 사진을 통해 서울과 서울 사람들의 역사를 한눈에 볼 수 있는 ‘2013 서울사진축제’(서울시립미술관) 등도 이어진다. ○48억원짜리 사진도 등장사진은 실험성과 도전 정신을 바탕으로 한 현대미술 장르다. 20~40대 영상세대를 중심으로 사진 애호가층이 두터워지면서 해마다 작품전이 늘고, 작가도 증가하는 추세다. 가격은 그림보다 싼 편이다. 컬렉터들이 유망한 작가들의 사진 작품을 많이 찾는 이유도 여기에 있다. 2011년 11월 뉴욕 크리스티 경매에서 안드레아스 거스키 작품 ‘라인강2’가 무려 433만달러(약 48억원)에 낙찰돼 사진 분야 최고 경매 낙찰가를 기록하기도 했다. 앞서 5월에는 신디 셔먼의 ‘무제#96’이 389만달러에 팔려나갔다. 사진평론가 김남진 씨는 “최근 사진 예술은 조각, 영상, 회화 등 다른 장르를 넘나들면서 새로운 미학을 만들어 내고 있다”며 “다이내믹한 에너지를 뿜어내는 거장들의 독특한 테마 작품을 주목할 만하다”고 말했다.\n",
            "Question: 한미사진미술관에서 프랭크의 작품을 마지막으로 감상할 수 있는 때는?\n",
            "#2 545 552 [(0, 2), (2, 4), (4, 5), (6, 9), (9, 10), (11, 13), (13, 14), (14, 15), (16, 19), (20, 23), (23, 24), (25, 26), (26, 29), (30, 32), (32, 33), (34, 37), (37, 38), (39, 41), (42, 44), (45, 48), (49, 51), (51, 53), (53, 54), (55, 57), (57, 58), (58, 63), (64, 66), (66, 68), (69, 72), (73, 74), (74, 76), (76, 77), (78, 80), (80, 81), (81, 82), (83, 85), (85, 86), (87, 89), (89, 90), (91, 92), (92, 93), (94, 98), (99, 101), (101, 102), (103, 106), (106, 107), (108, 111), (111, 112), (113, 115), (115, 117), (118, 120), (120, 121), (122, 125), (126, 129), (129, 130), (130, 131), (132, 135), (136, 138), (138, 139), (139, 140), (141, 144), (145, 147), (148, 150), (150, 152), (152, 153), (154, 156), (156, 158), (159, 160), (160, 161), (162, 163), (163, 164), (165, 166), (166, 167), (167, 168), (169, 170), (170, 172), (172, 174), (174, 175), (176, 178), (178, 180), (181, 184), (185, 187), (188, 190), (191, 193), (193, 194), (195, 197), (198, 200), (200, 201), (201, 202), (202, 204), (205, 207), (208, 210), (210, 212), (212, 213), (214, 217), (218, 220), (220, 221), (222, 224), (224, 226), (227, 229), (229, 230), (231, 233), (233, 234), (235, 237), (237, 238), (239, 243), (243, 244), (245, 247), (247, 248), (249, 251), (251, 252), (253, 254), (255, 256), (256, 257), (257, 258), (259, 260), (260, 263), (263, 264), (264, 267), (268, 269), (270, 272), (273, 276), (276, 278), (278, 279), (280, 282), (282, 283), (284, 286), (286, 287), (288, 289), (290, 292), (292, 293), (294, 296), (296, 297), (297, 298), (299, 300), (300, 303), (303, 305), (305, 306), (307, 309), (309, 311), (312, 315), (315, 316), (316, 318), (318, 319), (319, 320), (321, 324), (324, 325), (326, 328), (329, 330), (330, 331), (332, 334), (335, 338), (339, 341), (341, 344), (345, 346), (346, 347), (347, 349), (350, 352), (352, 354), (354, 355), (356, 359), (359, 360), (361, 363), (363, 364), (365, 367), (367, 368), (368, 370), (371, 373), (373, 374), (375, 377), (377, 379), (379, 380), (381, 383), (383, 384), (385, 387), (387, 388), (388, 389), (390, 392), (392, 393), (394, 396), (396, 397), (398, 400), (400, 401), (402, 404), (404, 405), (405, 406), (407, 409), (410, 411), (412, 413), (413, 414), (415, 418), (418, 419), (420, 422), (423, 425), (425, 426), (427, 428), (428, 430), (431, 433), (433, 434), (435, 437), (437, 438), (439, 441), (441, 442), (443, 444), (445, 447), (447, 448), (448, 449), (449, 450), (451, 453), (454, 456), (456, 457), (457, 458), (459, 460), (460, 462), (462, 463), (464, 465), (465, 467), (467, 469), (469, 470), (471, 472), (472, 474), (474, 475), (476, 479), (480, 481), (482, 485), (485, 486), (486, 487), (487, 488), (489, 493), (493, 494), (494, 497), (498, 500), (501, 503), (503, 505), (506, 509), (509, 510), (510, 512), (512, 513), (513, 514), (515, 517), (517, 518), (518, 519), (520, 522), (523, 526), (527, 529), (529, 531), (531, 534), (534, 536), (537, 538), (538, 539), (540, 541), (541, 542), (542, 544), (545, 547), (548, 549), (549, 550), (550, 551), (551, 552), (552, 554), (555, 559), (559, 560), (561, 565), (565, 566), (566, 567), (568, 569), (569, 572), (572, 573), (573, 574), (575, 577), (577, 578), (579, 583), (583, 584), (584, 586), (586, 587), (587, 588), (589, 594), (595, 597), (598, 599), (600, 603), (603, 604), (605, 607), (607, 608), (609, 611), (612, 614), (614, 615), (616, 618), (618, 619), (620, 622), (622, 623), (623, 624), (625, 629), (630, 633), (634, 637), (637, 638), (638, 639), (640, 643), (643, 644), (645, 646), (646, 647), (647, 649), (650, 652), (653, 656), (657, 659), (659, 662), (662, 664), (665, 667), (667, 668), (669, 670), (670, 672), (672, 673), (674, 675), (676, 678), (678, 679), (680, 682), (682, 683), (683, 684), (684, 685), (686, 688), (689, 691), (692, 694), (694, 696), (697, 700), (701, 704), (704, 705), (705, 707), (707, 708), (708, 709), (710, 712), (713, 714), (715, 718), (718, 719), (719, 720), (720, 721), (722, 724), (724, 725), (726, 728), (728, 729), (730, 732), (732, 733), (734, 736), (736, 737), (738, 740), (741, 742), (743, 746), (746, 747), (747, 748), (749, 751), (751, 752), (753, 755), (755, 756), (756, 757), (758, 760), (760, 761), (762, 764), (764, 765), (766, 768), (768, 769), (769, 770), (771, 772), (772, 773), (773, 774), (775, 778), (779, 782), (782, 783), (783, 784), (784, 785), (786, 788), (789, 790), (791, 792), (792, 793), (793, 794), (795, 798), (798, 799), (800, 802), (802, 803), (803, 804), (805, 806), (806, 807), (808, 810), (810, 811), (812, 814), (815, 818), (818, 821), (821, 822), (823, 825), (825, 827), (827, 830), (830, 832), (833, 836), (836, 837), (838, 839), (839, 840), (841, 843), (844, 846), (846, 847), (847, 848), (849, 851), (851, 852), (853, 854), (854, 855), (856, 857), (857, 858), (858, 859), (860, 863), (864, 866), (866, 868), (869, 871), (871, 872), (873, 875), (875, 876), (876, 877), (878, 880), (880, 881), (882, 883), (883, 884), (885, 887), (887, 888), (888, 889), (890, 891), (891, 892), (893, 894), (894, 895), (895, 897), (898, 900), (901, 903), (903, 905), (905, 908), (908, 909), (910, 913), (913, 914), (915, 917), (917, 919), (920, 922), (922, 923), (924, 926), (926, 927), (928, 930), (930, 931), (932, 935), (936, 937), (937, 938), (939, 942), (942, 944), (945, 948), (949, 951), (951, 952), (953, 955), (955, 956), (957, 959), (959, 960), (961, 962), (963, 964), (964, 965), (966, 968), (969, 971), (971, 972), (972, 973), (973, 979), (979, 980), (980, 981), (982, 984), (984, 985), (985, 986), (987, 990), (990, 991), (991, 993), (993, 994), (994, 996), (997, 1000), (1001, 1004), (1004, 1006), (1006, 1009), (1009, 1010), (1010, 1011), (1012, 1015), (1015, 1016), (1016, 1018), (1018, 1019), (1019, 1021), (1021, 1025), (1025, 1026), (1026, 1027), (1028, 1031), (1031, 1032), (1032, 1033), (1033, 1035), (1035, 1036), (1037, 1038), (1038, 1039), (1040, 1043), (1043, 1044), (1045, 1047), (1047, 1048), (1049, 1051), (1052, 1054), (1054, 1055), (1056, 1058), (1059, 1061), (1061, 1062), (1062, 1063), (1064, 1066), (1066, 1067), (1068, 1070), (1070, 1071), (1072, 1073), (1074, 1075), (1076, 1077), (1077, 1078), (1079, 1080), (1080, 1084), (1085, 1087), (1087, 1089), (1089, 1091), (1091, 1093), (1093, 1095), (1095, 1097), (1097, 1100), (1100, 1101), (1102, 1103), (1103, 1104), (1105, 1109), (1109, 1110), (1111, 1112), (1112, 1114), (1114, 1115), (1115, 1116), (1116, 1118), (1119, 1121), (1121, 1122), (1123, 1125), (1125, 1127), (1127, 1128), (1129, 1131), (1131, 1132), (1132, 1133), (1134, 1136), (1137, 1139), (1139, 1140), (1141, 1143), (1143, 1145), (1146, 1147), (1148, 1150), (1150, 1152), (1153, 1155), (1155, 1156), (1156, 1157), (1158, 1160), (1160, 1161), (1161, 1163), (1163, 1164), (1165, 1167), (1167, 1169), (1169, 1170), (1171, 1173), (1173, 1175), (1176, 1178), (1179, 1182), (1182, 1183), (1183, 1184), (1185, 1188), (1188, 1189), (1189, 1191), (1192, 1193), (1193, 1195), (1196, 1198), (1198, 1199), (1199, 1200), (1201, 1202), (1202, 1203), (1203, 1204), (1205, 1207), (1207, 1208), (1209, 1211), (1211, 1212), (1212, 1213), (1214, 1216), (1216, 1217), (1217, 1218), (1219, 1221), (1221, 1222), (1223, 1225), (1225, 1227), (1228, 1229), (1230, 1231), (1231, 1232), (1232, 1233), (1233, 1234), (1235, 1238), (1238, 1239), (1239, 1240), (1241, 1243), (1243, 1244), (1245, 1247), (1247, 1248), (1248, 1249), (1250, 1252), (1253, 1255), (1255, 1256), (1257, 1259), (1260, 1261), (1261, 1262), (1263, 1265), (1265, 1266), (1267, 1269), (1269, 1270), (1271, 1272), (1272, 1273), (1273, 1274), (1275, 1279), (1279, 1280), (1281, 1283), (1283, 1284), (1285, 1287), (1288, 1292), (1293, 1295), (1295, 1297), (1298, 1303), (1304, 1307), (1308, 1310), (1311, 1312), (1312, 1315), (1315, 1316), (1316, 1317), (1317, 1318), (1319, 1321), (1322, 1325), (1325, 1326), (1326, 1328), (1328, 1329), (1329, 1330), (1331, 1333), (1333, 1334), (1334, 1335), (1335, 1336), (1336, 1337), (1338, 1340), (1340, 1341), (1342, 1344), (1345, 1347), (1348, 1350), (1351, 1353), (1354, 1357), (1357, 1358), (1359, 1361), (1361, 1362), (1362, 1363), (1363, 1364), (1365, 1366), (1366, 1367), (1367, 1368), (1369, 1371), (1372, 1373), (1373, 1374), (1374, 1375), (1375, 1376), (1377, 1379), (1380, 1382), (1382, 1383), (1384, 1385), (1385, 1387), (1387, 1388), (1388, 1390), (1390, 1391), (1391, 1392), (1393, 1396), (1396, 1397), (1397, 1399), (1399, 1400), (1401, 1405), (1405, 1406), (1406, 1407), (1408, 1410), (1410, 1413), (1414, 1417), (1418, 1419), (1419, 1420), (1421, 1422), (1422, 1424), (1425, 1427), (1428, 1430), (1430, 1431), (1432, 1434), (1434, 1435), (1436, 1438), (1438, 1439), (1440, 1442), (1443, 1444), (1445, 1447), (1448, 1450), (1450, 1451), (1452, 1455), (1455, 1457), (1458, 1461), (1462, 1464), (1464, 1465), (1466, 1468), (1468, 1469), (1470, 1471), (1471, 1472), (1473, 1474), (1474, 1475), (1475, 1476), (1476, 1477), (1478, 1479), (1479, 1483), (1483, 1484), (1485, 1488), (1488, 1489), (1490, 1493), (1493, 1494), (1495, 1497), (1497, 1498), (1498, 1499), (1500, 1502), (1502, 1503), (1504, 1506), (1507, 1509), (1509, 1510), (1511, 1513), (1513, 1514), (1515, 1516), (1516, 1517), (1517, 1518), (1518, 1519), (1519, 1520), (1521, 1522), (1522, 1523), (1523, 1524), (1524, 1525)]\n",
            "Answer: 내년 2월9일\n",
            "------2------\n",
            "Context: 최근 공개된 핀란드의 오디도서관에는 내년부터 스스로 도서관에서 책의 위치를 알아서 찾아가는 ‘로봇 사서’가 도입된다. 이 로봇은 자율주행을 갖춰 이용자들이 반납한 책을 싣고 길을 찾아 책장까지 나르는 기능을 갖췄다. 여기에는 로봇용 자율주행 기능이 적용된다. 다만, 이러한 자율주행 기능에는 아직까지는 초광대역통신(UWB), 혹은 무선랜(Wifi) 기술이 적용되어 실내에 별도 장치를 설치해야만 했다.  POSTECH(포항공대, 총장 김도연) 창의IT융합공학과 대학원생들이 지난 해 창업한 Polaris3D(대표 곽인범)가 컴퓨터 프로그램을 설치하는 것처럼 간단한 로봇 자율주행 솔루션을 개발, 8일(현지시간)부터 라스베이거스에서 열리는 세계 최대 가전‧IT제품 전시회 CES 2019에 출품한다.  Polaris3D는 로봇에 프로그램만 설치하면 공장이나 실내에서 자율주행할 수 있도록 하는 기술을 개발했으며, 이 기술을 이용하면 별도 시설을 설치할 필요가 없다.  이 프로그램은 로봇이 스스로 주행하며 지도를 작성하고, 이 지도를 기반으로 위치를 추정해 목표지점까지 주행하도록 한다. 연산량을 많이 사용하는 기존의 SLAM(Simultaneous Localization and Mapping) 기술 대신 측위(localization) 기술을 이용해 손바닥 보다 작은 공간에서도 움직일 수 있는 소형로봇이나 드론에도 활용할 수 있다.  이미 한국전자전(KES)를 비롯한 다양한 전자제품 전시회 출품은 물론, 지자체의 투자를 받은 Polaris3D는 올해 실내자율주행 솔루션이 필요한 로봇업체는 물론, 자율주행기술을 연구하는 연구실에 판매할 예정이다. 또한 여러 로봇업체들과 협력하여 물류로봇이나 지게차 등에 활용해 인프라 구축이 필요없는 자율주행 로봇을 선보일 계획을 가지고 있다.\n",
            "Question: Polaris3D에게 자본을 제공한 기관은?\n",
            "#2 732 735 [(0, 2), (3, 5), (5, 6), (7, 10), (10, 11), (12, 14), (14, 17), (17, 18), (18, 19), (20, 22), (22, 24), (25, 28), (29, 32), (32, 34), (35, 36), (36, 37), (38, 40), (40, 41), (42, 43), (43, 45), (46, 49), (49, 50), (51, 52), (52, 54), (55, 56), (56, 57), (57, 58), (58, 59), (60, 62), (62, 64), (64, 65), (66, 67), (68, 70), (70, 71), (72, 74), (74, 76), (76, 77), (78, 80), (81, 84), (84, 85), (85, 86), (87, 89), (89, 90), (91, 92), (92, 93), (94, 95), (95, 96), (97, 98), (98, 99), (100, 101), (101, 102), (103, 105), (105, 107), (108, 110), (110, 111), (112, 114), (114, 115), (116, 118), (118, 119), (119, 120), (121, 123), (123, 124), (124, 125), (126, 128), (128, 129), (130, 132), (132, 134), (135, 137), (137, 138), (139, 141), (141, 143), (143, 144), (145, 147), (147, 148), (149, 151), (151, 152), (153, 155), (155, 157), (158, 160), (160, 161), (161, 162), (163, 165), (165, 167), (167, 168), (169, 173), (173, 175), (175, 176), (176, 179), (179, 180), (180, 181), (182, 184), (185, 188), (188, 189), (189, 193), (193, 194), (195, 197), (197, 198), (199, 201), (201, 202), (202, 203), (204, 206), (206, 207), (208, 210), (211, 213), (213, 214), (215, 217), (217, 219), (219, 220), (221, 222), (222, 223), (223, 224), (226, 233), (233, 234), (234, 236), (236, 238), (238, 239), (240, 242), (243, 246), (246, 247), (248, 250), (250, 252), (252, 254), (254, 256), (256, 257), (258, 262), (262, 263), (263, 264), (265, 267), (268, 269), (270, 272), (272, 273), (274, 281), (281, 282), (282, 283), (283, 284), (284, 286), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (293, 296), (297, 301), (301, 302), (303, 305), (305, 306), (306, 307), (308, 309), (309, 311), (312, 314), (314, 315), (316, 318), (319, 321), (321, 323), (324, 327), (327, 328), (329, 331), (331, 332), (333, 334), (334, 335), (335, 336), (336, 338), (338, 340), (340, 341), (341, 343), (344, 350), (350, 352), (353, 355), (355, 356), (357, 359), (360, 362), (363, 365), (365, 366), (366, 368), (368, 370), (371, 374), (375, 378), (379, 383), (383, 384), (385, 387), (387, 389), (389, 390), (392, 399), (399, 400), (400, 401), (401, 402), (403, 405), (405, 406), (407, 411), (411, 412), (413, 415), (415, 416), (416, 417), (418, 420), (420, 422), (423, 425), (425, 427), (428, 430), (430, 432), (432, 433), (434, 435), (436, 437), (437, 439), (440, 441), (441, 442), (443, 445), (445, 446), (447, 449), (449, 452), (452, 453), (454, 455), (456, 458), (458, 459), (460, 462), (462, 463), (463, 464), (465, 467), (468, 470), (470, 471), (472, 474), (474, 475), (476, 478), (478, 479), (480, 481), (481, 482), (482, 483), (485, 486), (487, 491), (491, 492), (493, 495), (495, 496), (497, 500), (501, 503), (503, 504), (504, 505), (506, 508), (508, 509), (510, 512), (512, 513), (513, 514), (514, 515), (516, 517), (518, 520), (520, 521), (522, 524), (524, 526), (527, 529), (529, 530), (531, 533), (533, 534), (535, 538), (538, 539), (539, 541), (542, 544), (544, 545), (545, 547), (548, 550), (550, 551), (552, 554), (554, 555), (555, 556), (557, 559), (560, 562), (562, 563), (563, 564), (565, 567), (567, 568), (569, 573), (573, 574), (574, 586), (587, 599), (600, 603), (604, 611), (611, 612), (613, 615), (616, 618), (619, 621), (621, 622), (622, 634), (634, 635), (636, 638), (638, 639), (640, 642), (642, 643), (644, 647), (648, 650), (651, 652), (652, 653), (654, 656), (656, 658), (658, 659), (660, 663), (664, 665), (666, 667), (667, 668), (669, 671), (671, 673), (673, 675), (676, 678), (678, 679), (679, 680), (681, 683), (683, 684), (685, 686), (687, 688), (688, 689), (689, 690), (692, 694), (695, 700), (700, 701), (701, 704), (704, 705), (705, 706), (707, 709), (709, 710), (711, 713), (713, 714), (715, 717), (717, 719), (720, 723), (724, 726), (726, 727), (728, 730), (730, 731), (732, 735), (735, 736), (737, 739), (739, 740), (741, 742), (742, 743), (744, 751), (751, 752), (752, 753), (753, 754), (755, 757), (758, 760), (760, 762), (762, 764), (765, 768), (768, 769), (770, 772), (772, 773), (774, 776), (776, 778), (778, 779), (780, 782), (782, 783), (784, 786), (786, 788), (788, 790), (790, 791), (792, 794), (794, 795), (795, 796), (797, 800), (800, 801), (802, 804), (804, 805), (806, 808), (808, 809), (809, 810), (810, 811), (812, 814), (815, 817), (818, 820), (820, 822), (822, 823), (823, 824), (825, 827), (827, 828), (828, 829), (830, 832), (832, 834), (834, 836), (837, 840), (841, 842), (842, 843), (844, 846), (846, 847), (848, 851), (852, 854), (854, 855), (856, 858), (858, 859), (859, 860), (861, 863), (863, 865), (866, 868), (868, 869), (870, 873), (874, 876), (876, 877), (878, 880), (880, 881), (882, 883), (883, 884), (884, 885)]\n",
            "Answer: 지자체\n",
            "------3------\n",
            "Context: 향수로 20대의 생기 있는 향을 남기고 싶어 하는 대학생이 많은 것으로 나타났다. ‘캠퍼스 잡앤조이’가 대학생 기자단과 모니터단을 대상으로 온라인 설문조사한 결과 응답자 251명 중 75.2%가 향수를 쓰고 있다고 응답했다.향수를 쓰는 대학생 가운데 여대생이 가장 좋아하는 향수 브랜드는 ‘랑방’(36.4%)인 것으로 조사됐다. 랑방은 20대를 타깃으로 하는 향수인 만큼 상큼하고 달콤한 향을 가진 제품라인이 인기를 얻고 있다는 분석이다. 그중 ‘에끌라 드 아르페쥬’(사진)와 ‘메리미’가 베스트셀러다.여대생 선호 향수 2위는 ‘디올’(14.5%)이었고, 이어 ‘안나수이’(11.9%) ‘마크제이콥스’(9.9%) 불가리·제니퍼로페즈(각 7.9%) 순이었다.남학생들이 가장 선호하는 향수 브랜드는 ‘CK’(18.4%)였으며, 그중에서도 ‘여자들이 좋아하는 남자 향수’로 알려진 ‘CK ONE’은 깔끔한 향기와 저렴한 가격으로 대학생들에게 인기를 끌고 있는 것으로 나타났다. 이어 남학생 선호 향수는 ‘불가리’(15.7%) ‘존 바바토스’(13.1%) 베르사체(10.5%) 등이었다.이런 향수 제품을 구입할 수 있는 헬스·뷰티숍 가운데 대학생이 가장 많이 찾는 곳으로는 ‘CJ올리브영’(85.2%)이 1위를 차지했다. 헬스·뷰티숍 2위와 3위는 ‘GS 왓슨스’(7.9%)와 ‘신세계 분스’(3.1%)였다.\n",
            "Question: 남학생 선호 2순위 향수 브랜드는 무엇인가?\n",
            "#2 502 508 [(0, 2), (2, 3), (4, 6), (6, 7), (7, 8), (9, 11), (12, 13), (13, 14), (15, 16), (16, 17), (18, 20), (20, 21), (22, 23), (23, 24), (25, 26), (26, 27), (28, 31), (31, 32), (33, 34), (34, 35), (36, 37), (37, 39), (40, 43), (43, 44), (44, 45), (46, 47), (47, 50), (51, 52), (52, 53), (53, 55), (55, 56), (56, 57), (58, 61), (62, 65), (65, 66), (67, 70), (70, 71), (71, 72), (73, 75), (75, 77), (78, 81), (82, 84), (84, 86), (86, 87), (88, 90), (91, 94), (95, 98), (98, 99), (100, 101), (102, 104), (104, 105), (105, 106), (106, 107), (107, 108), (109, 111), (111, 112), (113, 114), (114, 115), (116, 117), (117, 119), (120, 122), (122, 123), (123, 124), (124, 125), (125, 127), (127, 128), (129, 130), (130, 131), (132, 135), (136, 139), (140, 143), (143, 144), (145, 147), (148, 149), (149, 150), (150, 151), (151, 152), (153, 155), (156, 159), (159, 160), (161, 162), (162, 164), (164, 166), (166, 168), (168, 169), (169, 170), (170, 172), (172, 173), (174, 175), (175, 177), (178, 180), (180, 181), (181, 182), (182, 183), (184, 186), (186, 187), (188, 190), (190, 191), (191, 192), (193, 195), (195, 197), (198, 199), (199, 200), (201, 203), (203, 204), (205, 207), (208, 210), (210, 211), (211, 212), (213, 215), (215, 216), (217, 218), (218, 219), (220, 222), (223, 225), (225, 227), (227, 228), (229, 231), (231, 232), (233, 234), (234, 235), (236, 237), (237, 239), (240, 242), (242, 243), (243, 244), (244, 245), (246, 247), (247, 248), (249, 250), (250, 251), (251, 252), (252, 253), (254, 255), (256, 258), (258, 260), (260, 262), (262, 264), (264, 265), (265, 266), (267, 268), (268, 270), (270, 271), (271, 272), (272, 273), (274, 279), (279, 280), (280, 281), (281, 284), (285, 287), (288, 290), (291, 292), (292, 293), (293, 294), (295, 296), (296, 297), (297, 298), (298, 300), (300, 302), (302, 303), (303, 304), (304, 306), (306, 307), (307, 308), (308, 309), (309, 310), (311, 313), (314, 315), (315, 317), (317, 319), (319, 321), (321, 323), (323, 324), (324, 325), (325, 327), (328, 329), (329, 335), (335, 337), (337, 338), (338, 339), (339, 340), (340, 342), (343, 346), (346, 347), (347, 353), (353, 354), (354, 355), (356, 357), (357, 358), (358, 359), (359, 361), (362, 363), (363, 364), (364, 365), (365, 366), (366, 367), (367, 370), (370, 371), (371, 372), (373, 375), (376, 378), (378, 379), (379, 380), (381, 383), (384, 387), (387, 388), (389, 390), (390, 392), (392, 394), (394, 396), (396, 397), (397, 398), (398, 400), (400, 403), (403, 404), (405, 406), (406, 407), (407, 409), (409, 410), (411, 412), (412, 414), (414, 415), (415, 416), (417, 418), (418, 419), (419, 420), (420, 421), (422, 424), (425, 427), (427, 428), (428, 429), (430, 433), (434, 435), (435, 437), (438, 441), (441, 442), (442, 443), (444, 446), (446, 447), (448, 450), (450, 451), (452, 454), (454, 455), (456, 458), (458, 460), (461, 464), (464, 465), (465, 467), (468, 470), (470, 471), (472, 473), (473, 474), (475, 476), (476, 477), (478, 479), (479, 481), (482, 485), (485, 486), (486, 487), (488, 490), (491, 494), (495, 497), (498, 500), (500, 501), (502, 503), (503, 506), (506, 508), (508, 510), (510, 511), (511, 512), (512, 514), (515, 516), (516, 517), (518, 520), (520, 522), (522, 524), (524, 526), (526, 527), (527, 528), (528, 530), (531, 535), (535, 536), (536, 538), (538, 539), (539, 540), (540, 542), (543, 544), (544, 545), (545, 546), (546, 547), (547, 548), (548, 550), (551, 553), (554, 556), (556, 557), (558, 560), (560, 561), (562, 563), (564, 565), (565, 566), (567, 569), (569, 570), (570, 572), (572, 573), (574, 577), (578, 581), (581, 582), (583, 585), (586, 588), (589, 590), (590, 591), (592, 593), (593, 595), (595, 596), (597, 598), (598, 600), (600, 603), (603, 604), (604, 606), (606, 608), (608, 609), (609, 610), (610, 612), (612, 613), (614, 615), (615, 616), (616, 617), (618, 620), (620, 621), (621, 622), (622, 623), (624, 626), (626, 627), (627, 629), (629, 630), (631, 632), (632, 633), (633, 634), (635, 636), (636, 637), (637, 638), (639, 640), (640, 642), (643, 645), (645, 646), (646, 648), (648, 649), (649, 650), (650, 651), (651, 653), (653, 654), (655, 656), (656, 659), (660, 661), (661, 662), (662, 664), (664, 665), (665, 666), (666, 667), (667, 669), (669, 670), (670, 671), (671, 672)]\n",
            "Answer: ‘불가리’(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/My Drive/KoreanMRC/ko_mrc/utils.py:104: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  start = index // len(end_prob)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQpvytjDklp9"
      },
      "source": [
        "test_dataset = datasets.TokenizedKoMRC.load('data/test.json')\n",
        "test_dataset = utils.TokenizerWrapperDataset(test_dataset, tokenizer)\n",
        "print(\"Number of Questions\", len(test_dataset))\n",
        "print(test_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2-PdDCDmqtO"
      },
      "source": [
        "rows = []\n",
        "for raw_sample in tqdm(test_dataset, \"Testing\"):\n",
        "        sample = dict(raw_sample) \n",
        "        guid = sample.pop('guid')\n",
        "        context = sample.pop('context')\n",
        "        position = sample.pop('position')\n",
        "        start, end = sample.pop('start'), sample.pop('end')\n",
        "        del sample['question']\n",
        "\n",
        "        sample = {key: value.cuda()[None, :] for key, value in sample.items()}\n",
        "        start_logits, end_logits = model(**sample)\n",
        "        start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
        "    \n",
        "        rows.append([guid, tokenizer.logits2answer(raw_sample, start_logits, end_logits)])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hktCRim2klp-"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "os.makedirs('out', exist_ok=True)\n",
        "with torch.no_grad(), open('out/submission.csv', 'w') as fd:\n",
        "    writer = csv.writer(fd)\n",
        "    writer.writerow(['Id', 'Predicted'])\n",
        "\n",
        "    rows = []\n",
        "    for raw_sample in tqdm(test_dataset, \"Testing\"):\n",
        "        sample = dict(raw_sample) \n",
        "        guid = sample.pop('guid')\n",
        "        context = sample.pop('context')\n",
        "        position = sample.pop('position')\n",
        "        start, end = sample.pop('start'), sample.pop('end')\n",
        "        del sample['question']\n",
        "\n",
        "        sample = {key: value.cuda()[None, :] for key, value in sample.items()}\n",
        "        start_logits, end_logits = model(**sample)\n",
        "        start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
        "\n",
        "        rows.append([guid, tokenizer.logits2answer(raw_sample, start_logits, end_logits)])\n",
        "    \n",
        "    writer.writerows(rows)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}